{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Byte Pair Encoding of Unknown Words  \n",
    "\n",
    "Try the BPE tokenizer from the `tiktoken` library on the unknown words **\"Akwirw ier\"** and print the individual token IDs. Then, call the `decode` function on each resulting integer to reproduce the mapping shown in **Figure 2.11**. Lastly, call the `decode` method on the token IDs to check if it can reconstruct the original input, **\"Akwirw ier\"**.  \n",
    "\n",
    "A detailed discussion and implementation of BPE is beyond the scope of this book. However, in short, BPE builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.  \n",
    "\n",
    "For example, BPE starts by adding all individual characters to its vocabulary (`\"a\"`, `\"b\"`, ...). Then, it merges frequently occurring character combinations into subwords. For instance, `\"d\"` and `\"e\"` may be merged into the subword `\"de\"`, which is common in words like **\"define\"**, **\"depend\"**, **\"made\"**, and **\"hidden\"**. These merges are determined by a frequency cutoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Data Loaders with Different Strides and Context Sizes  \n",
    "\n",
    "To better understand how the data loader works, try running it with different settings, such as:  \n",
    "- `max_length=2` and `stride=2`  \n",
    "- `max_length=8` and `stride=2`  \n",
    "\n",
    "Batch sizes of `1`—as we have used so far—are useful for illustration. If you have experience with deep learning, you may know that smaller batch sizes require less memory but lead to noisier model updates. As in regular deep learning, **batch size is a trade-off and a hyperparameter** to experiment with when training LLMs.  \n",
    "\n",
    "Before moving on to the final sections of this chapter (which focus on creating embedding vectors from token IDs), let's briefly explore sampling with a batch size greater than `1`:  \n",
    "\n",
    "```python\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "```\n",
    "\n",
    "This produces the following output:\n",
    "\n",
    "#### Inputs:\n",
    "```\n",
    "tensor([[   40,   367,  2885,  1464],\n",
    "        [ 1807,  3619,   402,   271],\n",
    "        [10899,  2138,   257,  7026],\n",
    "        [15632,   438,  2016,   257],\n",
    "        [  922,  5891,  1576,   438],\n",
    "        [  568,   340,   373,   645],\n",
    "        [ 1049,  5975,   284,   502],\n",
    "        [  284,  3285,   326,    11]])\n",
    "```\n",
    "\n",
    "#### Targets:\n",
    "```\n",
    "tensor([[  367,  2885,  1464,  1807],\n",
    "        [ 3619,   402,   271, 10899],\n",
    "        [ 2138,   257,  7026, 15632],\n",
    "        [  438,  2016,   257,   922],\n",
    "        [ 5891,  1576,   438,   568],\n",
    "        [  340,   373,   645,  1049],\n",
    "        [ 5975,   284,   502,   284],\n",
    "        [ 3285,   326,    11,   287]])\n",
    "```\n",
    "\n",
    "Note that we **increase the stride to 4**. This ensures full dataset utilization (without skipping words) while avoiding excessive overlap between batches, which could otherwise lead to overfitting.  \n",
    "\n",
    "In the final two sections of this chapter, we will implement **embedding layers** that convert token IDs into continuous vector representations—an essential input format for LLMs.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
